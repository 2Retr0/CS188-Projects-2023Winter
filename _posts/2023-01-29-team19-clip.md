---
layout: post
comments: true
title: Investidation on the limitation of CLIP
author: Mengran (Diana) Dai, Yupei Hu
date: 2022-01-29
---

> Topic: CLIP: Text-image joint embedding

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Abstract
> **Our project topic is CLIP: Text-image joint embedding. CLIP is a neural network created by open.ai that can effectively learn visual concepts from natural language supervision.Since deep learning needs a lot of data, same as vision models, we try to use the photos that are already on the internet, reducing the cost of our training. Also, CLIP can be used for classification tasks for “out of the box” tasks by simply telling the model the visual concepts of the task. However, CLIP still has poor generalization to images that are not included in the pre-training dataset; therefore, we will investigate ways to improve the generalization in the model and narrow down the limitations.**

  
 
## Reference
  
Learning Transferable Visual Models From Natural Language Supervision

[http://proceedings.mlr.press/v139/radford21a/radford21a.pdf](http://proceedings.mlr.press/v139/radford21a/radford21a.pdf)

 
Language Models are Few-Shot Learners

[https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)


Self-training with Noisy Student improves ImageNet classification

[https://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_Self-Training_With_Noisy_Student_Improves_ImageNet_Classification_CVPR_2020_paper.pdf](https://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_Self-Training_With_Noisy_Student_Improves_ImageNet_Classification_CVPR_2020_paper.pdf)
[1] Redmon, Joseph, et al. "You only look once: Unified, real-time object detection." *Proceedings of the IEEE conference on computer vision and pattern recognition*. 2016.

  

---